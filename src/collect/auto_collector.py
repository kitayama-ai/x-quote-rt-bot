"""
X Auto Post System â€” è‡ªå‹•ãƒã‚ºãƒ„ã‚¤ãƒ¼ãƒˆåé›†ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³Bï¼‰

X API v2 (tweepy) ã‚’ä½¿ã£ã¦ã€target_accounts.json ã«ç™»éŒ²ã•ã‚ŒãŸ
æµ·å¤–AIã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒã‚ºãƒ„ã‚¤ãƒ¼ãƒˆã‚’è‡ªå‹•åé›†ã—ã€ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã™ã‚‹ã€‚

Usage:
    python -m src.main collect --account 1 [--dry-run]
"""
import json
from datetime import datetime, timedelta
from pathlib import Path
from zoneinfo import ZoneInfo

from src.collect.x_api_client import XAPIClient, XAPIError
from src.collect.tweet_parser import TweetParser, ParsedTweet
from src.collect.queue_manager import QueueManager
from src.collect.preference_scorer import PreferenceScorer
from src.config import PROJECT_ROOT

JST = ZoneInfo("Asia/Tokyo")

# 1å›ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã«å«ã‚ã‚‹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ•°ã®ä¸Šé™
# ORçµåˆãŒå¤šã™ãã‚‹ã¨APIå´ã§å¼¾ã‹ã‚Œã‚‹å ´åˆãŒã‚ã‚‹
MAX_ACCOUNTS_PER_QUERY = 8


class AutoCollector:
    """ãƒã‚ºãƒ„ã‚¤ãƒ¼ãƒˆã‚’è‡ªå‹•åé›†ã—ã¦ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ """

    def __init__(
        self,
        bearer_token: str = "",
        queue: QueueManager | None = None,
    ):
        self.client = XAPIClient(bearer_token=bearer_token)
        self.queue = queue or QueueManager()
        self.preference_scorer = PreferenceScorer()
        self._load_config()

    def _load_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
        path = PROJECT_ROOT / "config" / "target_accounts.json"
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        self.target_accounts = data.get("accounts", [])
        self.keywords = data.get("keywords", [])

        # å¼•ç”¨RTãƒ«ãƒ¼ãƒ«ï¼ˆãƒã‚ºé–¾å€¤ï¼‰
        rules_path = PROJECT_ROOT / "config" / "quote_rt_rules.json"
        with open(rules_path, "r", encoding="utf-8") as f:
            rules = json.load(f)
        self.buzz_thresholds = rules.get("buzz_thresholds", {})

    def collect(
        self,
        min_likes: int | None = None,
        lang: str | None = None,
        max_age_hours: int | None = None,
        max_tweets: int = 50,
        auto_approve: bool = False,
        dry_run: bool = False,
    ) -> dict:
        """
        ãƒã‚ºãƒ„ã‚¤ãƒ¼ãƒˆã‚’åé›†ã—ã¦ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 

        Args:
            min_likes: æœ€ä½ã„ã„ã­æ•°ï¼ˆNone=è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼‰
            lang: è¨€èªãƒ•ã‚£ãƒ«ã‚¿
            max_age_hours: æœ€å¤§ãƒ„ã‚¤ãƒ¼ãƒˆçµŒéæ™‚é–“
            max_tweets: æœ€å¤§å–å¾—ä»¶æ•°
            auto_approve: è‡ªå‹•æ‰¿èªã™ã‚‹ã‹
            dry_run: True=ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã—ãªã„

        Returns:
            {"fetched", "filtered", "added", "skipped_dup", "tweets"}
        """
        _min_likes = min_likes or self.buzz_thresholds.get("likes_min", 500)
        _lang = lang or self.buzz_thresholds.get("lang", ["en"])[0]
        _max_age = max_age_hours or self.buzz_thresholds.get("age_max_hours", 48)

        print(f"ğŸ” åé›†è¨­å®š: min_likes={_min_likes}, lang={_lang}, max_age={_max_age}h")

        # â”€â”€ STEP 1: APIæ¤œç´¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        raw_tweets = self._fetch_tweets(
            min_likes=_min_likes,
            lang=_lang,
            max_tweets=max_tweets,
        )
        print(f"ğŸ“¥ APIå–å¾—: {len(raw_tweets)}ä»¶")

        # â”€â”€ STEP 2: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        filtered = self._filter_tweets(
            raw_tweets,
            min_likes=_min_likes,
            max_age_hours=_max_age,
        )
        print(f"ğŸ” ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(filtered)}ä»¶ ({len(raw_tweets) - len(filtered)}ä»¶é™¤å¤–)")

        # â”€â”€ STEP 3: ParsedTweetã«å¤‰æ› â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        parsed_tweets = []
        for tweet_data in filtered:
            try:
                parsed = TweetParser.from_api_data(tweet_data, source="x_api_v2")
                parsed_tweets.append(parsed)
            except Exception as e:
                print(f"  âš ï¸ ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")

        # â”€â”€ STEP 3.5: ãƒ—ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for tweet in parsed_tweets:
            pref_result = self.preference_scorer.score(
                tweet_text=tweet.text,
                author_username=tweet.author_username,
            )
            tweet.preference_match_score = pref_result["preference_score"]
            tweet.matched_topics = pref_result["matched_topics"]
            tweet.matched_keywords = pref_result["matched_keywords"]

        # ãƒ—ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã§ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸãƒ„ã‚¤ãƒ¼ãƒˆã‚’é™¤å¤–
        before_pref = len(parsed_tweets)
        parsed_tweets = [
            t for t in parsed_tweets
            if not self.preference_scorer.is_account_blocked(t.author_username)
        ]
        if before_pref != len(parsed_tweets):
            print(f"ğŸš« ãƒ–ãƒ­ãƒƒã‚¯ã‚¢ã‚«ã‚¦ãƒ³ãƒˆé™¤å¤–: {before_pref - len(parsed_tweets)}ä»¶")

        # ãƒ–ãƒ¬ãƒ³ãƒ‰ã‚¹ã‚³ã‚¢ï¼ˆã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ Ã— ãƒ—ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ï¼‰ã§å†ã‚½ãƒ¼ãƒˆ
        parsed_tweets.sort(
            key=lambda t: (t.likes + t.retweets * 3) * max(t.preference_match_score, 0.1),
            reverse=True,
        )

        pref_matched = sum(1 for t in parsed_tweets if t.preference_match_score > 1.0)
        print(f"ğŸ¯ ãƒ—ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ãƒãƒƒãƒ: {pref_matched}/{len(parsed_tweets)}ä»¶")

        # â”€â”€ STEP 4: ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        added = 0
        skipped_dup = 0

        if not dry_run:
            for tweet in parsed_tweets:
                if self.queue.add(tweet):
                    added += 1
                    if auto_approve:
                        self.queue.approve(tweet.tweet_id)
                else:
                    skipped_dup += 1
        else:
            added = len(parsed_tweets)

        return {
            "fetched": len(raw_tweets),
            "filtered": len(filtered),
            "added": added,
            "skipped_dup": skipped_dup,
            "tweets": parsed_tweets,
        }

    def _fetch_tweets(
        self,
        min_likes: int,
        lang: str,
        max_tweets: int,
    ) -> list[dict]:
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‹ã‚‰ãƒã‚ºãƒ„ã‚¤ãƒ¼ãƒˆã‚’æ¤œç´¢"""
        all_tweets = []

        # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’priorityé †ã«ã‚½ãƒ¼ãƒˆï¼ˆhighå„ªå…ˆï¼‰
        priority_order = {"high": 0, "medium": 1, "low": 2}
        sorted_accounts = sorted(
            self.target_accounts,
            key=lambda a: priority_order.get(a.get("priority", "medium"), 1),
        )

        # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
        usernames = [a["username"] for a in sorted_accounts]
        chunks = [
            usernames[i : i + MAX_ACCOUNTS_PER_QUERY]
            for i in range(0, len(usernames), MAX_ACCOUNTS_PER_QUERY)
        ]

        for chunk in chunks:
            query = self.client.build_search_query(
                accounts=chunk,
                min_likes=min_likes,
                lang=lang,
            )
            print(f"  ğŸ” æ¤œç´¢: {query[:80]}...")

            try:
                tweets = self.client.search_tweets(
                    query=query,
                    max_results=max_tweets,
                )
                all_tweets.extend(tweets)
                print(f"     â†’ {len(tweets)}ä»¶å–å¾—")
            except XAPIError as e:
                print(f"     âŒ APIã‚¨ãƒ©ãƒ¼: {e}")

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ¤œç´¢ã§ååˆ†å–ã‚Œãªã‹ã£ãŸå ´åˆã®è£œå®Œï¼‰
        if len(all_tweets) < max_tweets // 2 and self.keywords:
            query = self.client.build_search_query(
                keywords=self.keywords[:5],
                min_likes=min_likes,
                lang=lang,
            )
            print(f"  ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢: {query[:80]}...")

            try:
                tweets = self.client.search_tweets(
                    query=query,
                    max_results=max_tweets // 2,
                )
                all_tweets.extend(tweets)
                print(f"     â†’ {len(tweets)}ä»¶å–å¾—")
            except XAPIError as e:
                print(f"     âŒ APIã‚¨ãƒ©ãƒ¼: {e}")

        # é‡è¤‡æ’é™¤ï¼ˆåŒã˜tweet_idãŒè¤‡æ•°ã‚¯ã‚¨ãƒªã§å–ã‚Œã‚‹å ´åˆï¼‰
        seen_ids = set()
        unique = []
        for t in all_tweets:
            tid = str(t.get("id_str", t.get("id", "")))
            if tid and tid not in seen_ids:
                seen_ids.add(tid)
                unique.append(t)

        return unique

    def _filter_tweets(
        self,
        tweets: list[dict],
        min_likes: int,
        max_age_hours: int,
    ) -> list[dict]:
        """ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        now = datetime.now(JST)
        cutoff = now - timedelta(hours=max_age_hours)
        filtered = []

        # ã‚­ãƒ¥ãƒ¼ã«ã‚ã‚‹æ—¢å­˜ãƒ„ã‚¤ãƒ¼ãƒˆIDã‚’å–å¾—ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
        existing_ids = set()
        try:
            all_pending = self.queue.get_all_pending()
            existing_ids = {item["tweet_id"] for item in all_pending}
            processed = self.queue._load(self.queue._processed_file)
            existing_ids.update(item["tweet_id"] for item in processed)
        except Exception:
            pass

        # åŒä¸€ã‚½ãƒ¼ã‚¹åˆ¶é™: ä»Šæ—¥ã™ã§ã«è¿½åŠ æ¸ˆã¿ã®ã‚½ãƒ¼ã‚¹
        today_sources = set()
        try:
            today_str = now.date().isoformat()
            for item in all_pending:
                added_at = item.get("added_at", "")
                if added_at.startswith(today_str):
                    today_sources.add(item.get("author_username", ""))
        except Exception:
            pass

        for tweet in tweets:
            tid = str(tweet.get("id_str", tweet.get("id", "")))

            # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
            if tid in existing_ids:
                continue

            # ã„ã„ã­æ•°ãƒã‚§ãƒƒã‚¯
            likes = tweet.get("favorite_count", tweet.get("like_count", 0))
            if likes < min_likes:
                continue

            # è¨€èªãƒã‚§ãƒƒã‚¯ï¼ˆSocialDataã¯langãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿”ã™ï¼‰
            lang = tweet.get("lang", "")
            if lang and lang not in ("en", "und"):
                continue

            # ãƒªãƒ—ãƒ©ã‚¤é™¤å¤–
            if tweet.get("in_reply_to_status_id") or tweet.get("in_reply_to_status_id_str"):
                continue

            # RTé™¤å¤–
            if tweet.get("retweeted_status") or str(tweet.get("full_text", "")).startswith("RT @"):
                continue

            # åŒä¸€ã‚½ãƒ¼ã‚¹åˆ¶é™ï¼ˆ1æ—¥1ä»¶ã¾ã§ï¼‰
            user = tweet.get("user", {})
            username = user.get("screen_name", "")
            if username in today_sources:
                continue

            # çµŒéæ™‚é–“ãƒã‚§ãƒƒã‚¯ï¼ˆSocialDataã® created_at ã‚’ãƒ‘ãƒ¼ã‚¹ï¼‰
            created_at_str = tweet.get("created_at", "")
            if created_at_str:
                try:
                    # "Thu Feb 20 02:14:30 +0000 2026" å½¢å¼
                    created_at = datetime.strptime(
                        created_at_str, "%a %b %d %H:%M:%S %z %Y"
                    )
                    if created_at < cutoff:
                        continue
                except (ValueError, TypeError):
                    pass  # ãƒ‘ãƒ¼ã‚¹ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã›ãšé€šã™

            filtered.append(tweet)
            today_sources.add(username)

        # ã„ã„ã­æ•°ã®é™é †ã§ã‚½ãƒ¼ãƒˆ
        filtered.sort(
            key=lambda t: t.get("favorite_count", t.get("like_count", 0)),
            reverse=True,
        )

        return filtered

    def format_result(self, result: dict) -> str:
        """åé›†çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        lines = [
            "ğŸ“Š åé›†çµæœ:",
            f"  APIå–å¾—:    {result['fetched']}ä»¶",
            f"  ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {result['filtered']}ä»¶",
            f"  ã‚­ãƒ¥ãƒ¼è¿½åŠ : {result['added']}ä»¶",
            f"  é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {result['skipped_dup']}ä»¶",
        ]

        if result["tweets"]:
            lines.append("")
            lines.append("  ğŸ“ è¿½åŠ ã•ã‚ŒãŸãƒ„ã‚¤ãƒ¼ãƒˆ:")
            for t in result["tweets"][:10]:
                lines.append(
                    f"    @{t.author_username} ({t.likes:,}â¤) "
                    f"{t.text[:50]}..."
                )

        return "\n".join(lines)
