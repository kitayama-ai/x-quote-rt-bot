"""
X Auto Post System â€” ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿è‡ªå‹•æ›´æ–°

é€±æ¬¡åˆ†æã®çµæœã‹ã‚‰å‹ã¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡ºã—ã€ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•æ›´æ–°ã€‚
"""
import re
from datetime import datetime
from pathlib import Path
from zoneinfo import ZoneInfo

from src.config import Config, PROJECT_ROOT

JST = ZoneInfo("Asia/Tokyo")


class MasterUpdater:
    """ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•æ›´æ–°"""

    def __init__(self, config: Config):
        self.config = config

    def update_from_metrics(self, metrics: list[dict]) -> str:
        """
        ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ã€ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°ãƒ­ã‚°ã«è¿½è¨˜

        Args:
            metrics: MetricsCollector.collect_recent() ã®çµæœ

        Returns:
            æ›´æ–°å†…å®¹ã®èª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not metrics:
            return "ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ãªã—ã€‚æ›´æ–°ã‚¹ã‚­ãƒƒãƒ—ã€‚"

        # ãƒ™ã‚¹ãƒˆ/ãƒ¯ãƒ¼ã‚¹ãƒˆåˆ†æ
        sorted_by_engagement = sorted(
            metrics,
            key=lambda m: m.get("likes", 0) + m.get("retweets", 0) * 3,
            reverse=True
        )

        best_posts = sorted_by_engagement[:3]
        worst_posts = sorted_by_engagement[-3:]

        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        findings = []

        # ãƒ™ã‚¹ãƒˆæŠ•ç¨¿ã®å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        best_texts = [m.get("text", "") for m in best_posts]
        patterns = self._detect_patterns(best_texts)
        if patterns:
            findings.append(f"å‹ã¡ãƒ‘ã‚¿ãƒ¼ãƒ³: {', '.join(patterns)}")

        # ãƒ¯ãƒ¼ã‚¹ãƒˆæŠ•ç¨¿ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        worst_texts = [m.get("text", "") for m in worst_posts]
        anti_patterns = self._detect_patterns(worst_texts)
        if anti_patterns:
            findings.append(f"è² ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³: {', '.join(anti_patterns)}")

        # ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã«æ›´æ–°ãƒ­ã‚°ã‚’è¿½è¨˜
        today = datetime.now(JST).strftime("%Y/%m/%d")
        update_entry = f"| {today} | é€±æ¬¡åˆ†æ: {'; '.join(findings) if findings else 'ç‰¹ç­†äº‹é …ãªã—'} |"

        master_path = self.config.master_data_path
        with open(master_path, "r", encoding="utf-8") as f:
            content = f.read()

        # æ›´æ–°ãƒ­ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«ã®æœ€å¾Œã«è¿½è¨˜
        if "## æ›´æ–°ãƒ­ã‚°" in content:
            content = content.rstrip() + f"\n{update_entry}\n"
        else:
            content += f"\n\n## æ›´æ–°ãƒ­ã‚°\n\n| æ—¥ä»˜ | æ›´æ–°å†…å®¹ |\n|---|---|\n{update_entry}\n"

        with open(master_path, "w", encoding="utf-8") as f:
            f.write(content)

        summary = f"ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿æ›´æ–°å®Œäº†: {'; '.join(findings)}" if findings else "æ›´æ–°å†…å®¹ãªã—"
        print(f"ğŸ“ {summary}")
        return summary

    def _detect_patterns(self, texts: list[str]) -> list[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆç¾¤ã‹ã‚‰å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º"""
        patterns = []

        # æ›¸ãå‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³
        starts = []
        for text in texts:
            first_line = text.split('\n')[0] if text else ""
            if re.match(r'^(ã¶ã£ã¡ã‚ƒã‘|æ­£ç›´|ãƒã‚¸ã§)', first_line):
                starts.append("è‡ªå·±é–‹ç¤ºç³»ãƒ•ãƒƒã‚¯")
            elif re.match(r'^\d+', first_line):
                starts.append("æ•°å­—ãƒ•ãƒƒã‚¯")
            elif re.match(r'^(ã‚„ã°ã„|ãˆãã„|ã“ã‚Œ)', first_line):
                starts.append("æ„Ÿæƒ…ãƒ•ãƒƒã‚¯")

        if starts:
            most_common = max(set(starts), key=starts.count)
            patterns.append(f"ãƒ•ãƒƒã‚¯:{most_common}")

        # å…·ä½“æ€§ã®æœ‰ç„¡
        has_numbers = sum(1 for t in texts if re.search(r'\d+[ä¸‡å††%æ™‚é–“åˆ†]', t))
        if has_numbers >= 2:
            patterns.append("å…·ä½“çš„æ•°å­—ã‚ã‚Š")

        # é•·ã•åˆ†æ
        avg_len = sum(len(t.replace('\n', '')) for t in texts) / max(len(texts), 1)
        if avg_len < 140:
            patterns.append("çŸ­æ–‡(ã€œ140å­—)")
        elif avg_len > 220:
            patterns.append("é•·æ–‡(220å­—+)")

        return patterns
